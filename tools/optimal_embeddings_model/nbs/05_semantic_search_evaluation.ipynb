{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp semantic_search_evaluator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor/workspace/mailio-ai/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Set, List, Tuple\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from enum import Enum\n",
    "from torch import Tensor\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intfloat/e5-small-v2\n"
     ]
    }
   ],
   "source": [
    "model_id = os.getenv(\"MODEL_ID\")\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../data\"\n",
    "subfolder = model_id.split(\"/\")[-1]\n",
    "questions_file = f\"{base_dir}/evaluation_dataset/sample_queries_cleaned.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_queries = pd.read_json(questions_file, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARTIOM DRUMEA Sent You a Message Behance Basic...</td>\n",
       "      <td>&lt;010001809eb828f6-b2888a42-bd74-4f16-b366-cf0e...</td>\n",
       "      <td>\"message from ARTIOM DRUMEA on Behance\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fwd: CIIS Commencement 2021: Join us on May 1!...</td>\n",
       "      <td>&lt;BY5PR22MB208353712894331A58D4D4C6E72F9@BY5PR2...</td>\n",
       "      <td>\"CIIS Commencement 2021 details\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Your Digest: From the Mac Startup Tone to the ...</td>\n",
       "      <td>&lt;esuh8huNSw2sRLJpnIw4HQ@ismtpd0012p1iad2.sendg...</td>\n",
       "      <td>\"OneZero digital digest\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ARTIOM DRUMEA Sent You a Message Behance Basic...   \n",
       "1  Fwd: CIIS Commencement 2021: Join us on May 1!...   \n",
       "2  Your Digest: From the Mac Startup Tone to the ...   \n",
       "\n",
       "                                                  id  \\\n",
       "0  <010001809eb828f6-b2888a42-bd74-4f16-b366-cf0e...   \n",
       "1  <BY5PR22MB208353712894331A58D4D4C6E72F9@BY5PR2...   \n",
       "2  <esuh8huNSw2sRLJpnIw4HQ@ismtpd0012p1iad2.sendg...   \n",
       "\n",
       "                                     query  \n",
       "0  \"message from ARTIOM DRUMEA on Behance\"  \n",
       "1         \"CIIS Commencement 2021 details\"  \n",
       "2                 \"OneZero digital digest\"  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_queries.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index_path = f\"{base_dir}/{subfolder}/embeddings_index.npy\"\n",
    "embeddings_index = np.load(embeddings_index_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contruct the ground truth dictionary from the dataframe\n",
    "\n",
    "ground_truth = dict[int, Set[int]]()\n",
    "for idx, row in df_queries.iterrows():\n",
    "    email_id = row[\"id\"]\n",
    "    # find in embeddings index it's index\n",
    "    emb_index_set = set(np.where(embeddings_index == email_id)[0])\n",
    "    ground_truth[idx] = emb_index_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6391}\n",
      "[1320 1321 1322]\n"
     ]
    }
   ],
   "source": [
    "print(ground_truth[19])\n",
    "# embeddings_index[2712]\n",
    "print(np.where(embeddings_index == \"<CPZP284MB058472D9136C52B8957CB8B192C69@CPZP284MB0584.BRAP284.PROD.OUTLOOK.COM>\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SimilarityFunction(Enum):\n",
    "    COSINE = \"cosine\"\n",
    "    DOT_PRODUCT = \"dot\"\n",
    "    EUCLIDEAN = \"euclidean\"\n",
    "\n",
    "class MailioInformationRetrievalEvaluator:\n",
    "    \"\"\"\n",
    "    Insipired by on https://github.com/UKPLab/sentence-transformers/blob/v3.4-release/sentence_transformers/evaluation/InformationRetrievalEvaluator.py\n",
    "    Gives me a bit more le-way to customize creation of query embeddings and corpus embeddings\n",
    "    Also simplifies the code a bit\n",
    "\n",
    "    Given a set of queries and a large corpus set. It will retrieve for each query the top-k most similar document. It measures\n",
    "    Mean Reciprocal Rank (MRR), Recall@k, and Normalized Discounted Cumulative Gain (NDCG)\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        ground_truth:Dict[int, Set[int]],  # query_index => Set[corpus_index]\n",
    "        corpus_embeddings: Tensor, # embeddingns of corpus_index\n",
    "        query_embeddings: Tensor, # embeddings of query_index\n",
    "        mrr_at_k: List[int] = [10],\n",
    "        ndcg_at_k: List[int] = [10],\n",
    "        accuracy_at_k: List[int] = [1, 3, 5, 10],\n",
    "        precision_recall_at_k: List[int] = [1, 3, 5, 10],\n",
    "        map_at_k: List[int] = [100],\n",
    "        similarity_functions = [SimilarityFunction.COSINE],\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the InformationRetrievalEvaluator.\n",
    "        Args:\n",
    "            corpus_embeddings (Tensor): A tensor of shape (N, D) containing the document embeddings.\n",
    "            query_embeddings (Tensor): A tensor of shape (M, D) containing the query embeddings.\n",
    "            ground_truth (Dict[str, Set[str]]): A dictionary mapping query index to a set of relevant document indexes.\n",
    "            mrr_at_k (List[int]): A list of integers representing the values of k for MRR calculation. Defaults to [10].\n",
    "            ndcg_at_k (List[int]): A list of integers representing the values of k for NDCG calculation. Defaults to [10].\n",
    "            accuracy_at_k (List[int]): A list of integers representing the values of k for accuracy calculation. Defaults to [1, 3, 5, 10].\n",
    "            precision_recall_at_k (List[int]): A list of integers representing the values of k for precision and recall calculation. Defaults to [1, 3, 5, 10].\n",
    "            map_at_k (List[int]): A list of integers representing the values of k for MAP calculation. Defaults to [100].\n",
    "        \"\"\"\n",
    "        self.corpus_embeddings = corpus_embeddings\n",
    "        self.query_embeddings = query_embeddings\n",
    "        self.ground_truth = ground_truth\n",
    "        self.mrr_at_k = mrr_at_k\n",
    "        self.ndcg_at_k = ndcg_at_k\n",
    "        self.accuracy_at_k = accuracy_at_k\n",
    "        self.precision_recall_at_k = precision_recall_at_k\n",
    "        self.map_at_k = map_at_k\n",
    "        self.similarity_functions = similarity_functions\n",
    "        self.problematic_queries = set()\n",
    "\n",
    "    def run(self):\n",
    "        return self.compute_metrices()\n",
    "\n",
    "    def get_problematic_queries(self):\n",
    "        return self.problematic_queries\n",
    "\n",
    "    def compute_metrices(self):\n",
    "        \"\"\"\n",
    "        Computes the evaluation metrics.\n",
    "        Args:\n",
    "            top_k (int): The number of retrieved documents for which to compute the evaluation metrics. Defaults to 10.\n",
    "        Returns:\n",
    "            Dict[str, Dict[str, float]]: A dictionary mapping metric names to dictionaries mapping metric values to scores.\n",
    "        \"\"\"\n",
    "        max_k = max(\n",
    "            max(self.mrr_at_k),\n",
    "            max(self.ndcg_at_k),\n",
    "            max(self.accuracy_at_k),\n",
    "            max(self.precision_recall_at_k),\n",
    "            max(self.map_at_k),\n",
    "        )\n",
    "        # prepare the query result list for each query and each score function\n",
    "        self.queries_result_list = {}\n",
    "        \n",
    "        metrics = {}\n",
    "        for sim_fn in self.similarity_functions:\n",
    "            similarity_name = str(sim_fn.value)\n",
    "            queries_results = self.compute_similarity_function_product(sim_fn, top_k=max_k)\n",
    "            similarity_metrics = self.compute_metrics(queries_results)\n",
    "            metrics[similarity_name] = similarity_metrics\n",
    "        return metrics\n",
    "    \n",
    "    def compute_similarity_function_product(self, similarity_function: SimilarityFunction, top_k: int = 100) -> Dict[int, List[Tuple[float, int]]]:\n",
    "        \"\"\"\n",
    "        Computes the evaluation metrics for a given similarity function.\n",
    "        Args:\n",
    "            similarity_function (SimilarityFunction): The similarity function to use for computing the similarity between queries and documents.\n",
    "            top_k (int): The number of retrieved documents for which to compute the evaluation metrics. Defaults to 10.\n",
    "        Returns:\n",
    "            Dict[int, List[Tuple[float, int]]] : A dictionary mapping query indexes to a list of tuples containing the similarity score and the document index.\n",
    "        \"\"\"\n",
    "\n",
    "        query_results = {}\n",
    "        \n",
    "        # compute the similarity between each query and each document\n",
    "        if similarity_function == SimilarityFunction.COSINE:\n",
    "            for query_index in range(len(self.query_embeddings)):\n",
    "                query_embedding = self.query_embeddings[query_index]\n",
    "                similarity = F.cosine_similarity(query_embedding, self.corpus_embeddings, dim=1)\n",
    "                scores, indices = similarity.topk(top_k, dim=0)\n",
    "                s = scores.cpu().numpy().ravel()\n",
    "                i = indices.cpu().numpy().ravel()\n",
    "                query_results[query_index] = [(s, i) for s, i in zip(s, i)]\n",
    "        \n",
    "        if similarity_function == SimilarityFunction.DOT_PRODUCT:\n",
    "            for query_index in range(len(self.query_embeddings)):\n",
    "                query_embedding = self.query_embeddings[query_index]\n",
    "                similarity = torch.matmul(self.corpus_embeddings, query_embedding)\n",
    "                scores, indices = similarity.topk(top_k, dim=0)\n",
    "                s = scores.cpu().numpy().ravel()\n",
    "                i = indices.cpu().numpy().ravel()\n",
    "                query_results[query_index] = [(s, i) for s, i in zip(s, i)]\n",
    "\n",
    "        if similarity_function == SimilarityFunction.EUCLIDEAN:\n",
    "            for query_index in range(len(self.query_embeddings)):\n",
    "                query_embedding = self.query_embeddings[query_index]\n",
    "                similarity = torch.cdist(query_embedding.unsqueeze(0), self.corpus_embeddings, p=2).squeeze()\n",
    "                scores, indices = similarity.topk(top_k, dim=0, largest=False)\n",
    "                s = scores.cpu().numpy().ravel()\n",
    "                i = indices.cpu().numpy().ravel()\n",
    "                query_results[query_index] = [(s, i) for s, i in zip(s, i)]\n",
    "\n",
    "                \n",
    "        return query_results    \n",
    "\n",
    "    def compute_metrics(self, queries_results: Dict[int, List[Tuple[float, int]]]):\n",
    "        \"\"\"\n",
    "        Computes the evaluation metrics for a given similarity function.\n",
    "        Args:\n",
    "            queries_results (Dict[int, List[Tuple[float, int]]]): A dictionary mapping query indexes to a list of tuples containing the similarity score and the document index.\n",
    "        Returns:\n",
    "            Dict[str, Dict[str, float]]: A dictionary mapping metric names to dictionaries mapping metric values to scores.\n",
    "        \"\"\"\n",
    "        # Init score computation values\n",
    "        num_hits_at_k = {k: 0 for k in self.accuracy_at_k}\n",
    "        precisions_at_k = {k: [] for k in self.precision_recall_at_k}\n",
    "        recall_at_k = {k: [] for k in self.precision_recall_at_k}\n",
    "        MRR = {k: 0 for k in self.mrr_at_k}\n",
    "        ndcg = {k: [] for k in self.ndcg_at_k}\n",
    "        AveP_at_k = {k: [] for k in self.map_at_k}\n",
    "\n",
    "        # queries not in top 3\n",
    "        self.problematic_queries = set()\n",
    "\n",
    "        # Compute scores on results\n",
    "        for query_index, results in queries_results.items():\n",
    "            # Sort scores (probably unecessary but just in case)\n",
    "            # top_hits = sorted(results, key=lambda x: x[0], reverse=True)\n",
    "            top_hits = results\n",
    "\n",
    "            relevant_docs_ids = self.ground_truth[query_index]\n",
    "            # Accuracy@k - We count the result correct, if at least one relevant doc is across the top-k documents\n",
    "            found_any_acc_k = False\n",
    "            for k_val in self.accuracy_at_k:\n",
    "                for hit in top_hits[0:k_val]:\n",
    "                    if hit[1] in relevant_docs_ids:\n",
    "                        num_hits_at_k[k_val] += 1\n",
    "                        found_any_acc_k = True\n",
    "                        break\n",
    "            if not found_any_acc_k:\n",
    "                self.problematic_queries.add(query_index)\n",
    "\n",
    "            # Precision and Recall@k\n",
    "            for k_val in self.precision_recall_at_k:\n",
    "                num_correct = 0\n",
    "                for hit in top_hits[0:k_val]:\n",
    "                    if hit[1] in relevant_docs_ids:\n",
    "                        num_correct += 1\n",
    "                \n",
    "                precisions_at_k[k_val].append(num_correct / k_val)\n",
    "                recall_at_k[k_val].append(num_correct / len(relevant_docs_ids))\n",
    "\n",
    "            # @Mean Reciprocal Rank\n",
    "            for k_val in self.mrr_at_k:\n",
    "                for rank, hit in enumerate(top_hits[0:k_val]):\n",
    "                    if hit[1] in relevant_docs_ids:\n",
    "                        MRR[k_val] += 1.0 / (rank + 1)\n",
    "                        break\n",
    "\n",
    "            # NDCG@k (normalized discounted cumulative gain at k)\n",
    "            for k_val in self.ndcg_at_k:\n",
    "                dcg = 0\n",
    "                idcg = 0\n",
    "                for i in range(k_val):\n",
    "                    if i < len(top_hits):\n",
    "                        if top_hits[i][1] in relevant_docs_ids:\n",
    "                            dcg += 1 / np.log2(i + 2)\n",
    "                    idcg += 1 / np.log2(i + 2)\n",
    "                ndcg[k_val].append(dcg / idcg)\n",
    "            \n",
    "            # Map@k\n",
    "            for k_val in self.map_at_k:\n",
    "                num_correct = 0\n",
    "                sum_precisions = 0\n",
    "                for i, hit in enumerate(top_hits[0:k_val]):\n",
    "                    if hit[1] in relevant_docs_ids:\n",
    "                        num_correct += 1\n",
    "                        sum_precisions += num_correct / (i + 1)\n",
    "                avg_precision = sum_precisions / min(k_val, len(relevant_docs_ids))\n",
    "                AveP_at_k[k_val].append(avg_precision)\n",
    "\n",
    "            \n",
    "        # Compute averages\n",
    "        for k in num_hits_at_k:\n",
    "            num_hits_at_k[k] /= len(self.query_embeddings)\n",
    "\n",
    "        for k in precisions_at_k:\n",
    "            precisions_at_k[k] = np.mean(precisions_at_k[k])\n",
    "        \n",
    "        for k in recall_at_k:\n",
    "            recall_at_k[k] = np.mean(recall_at_k[k])\n",
    "\n",
    "        for k in MRR:\n",
    "            MRR[k] /= len(self.query_embeddings)\n",
    "\n",
    "        for k in ndcg:\n",
    "            ndcg[k] = np.mean(ndcg[k])\n",
    "        \n",
    "        for k in AveP_at_k:\n",
    "            AveP_at_k[k] = np.mean(AveP_at_k[k])\n",
    "        \n",
    "        return {\n",
    "            \"accuracy@k\": num_hits_at_k,\n",
    "            \"precision@k\": precisions_at_k,\n",
    "            \"recall@k\": recall_at_k,\n",
    "            \"ndcg@k\": ndcg,\n",
    "            \"mrr@k\": MRR,\n",
    "            \"map@k\": AveP_at_k,\n",
    "        }\n",
    "    \n",
    "    def output_scores(self, scores):\n",
    "        \"\"\"\n",
    "        Outputs the evaluation metrics.\n",
    "        Args:\n",
    "            metrics (Dict[str, Dict[str, float]]): A dictionary mapping metric names to dictionaries mapping metric values to scores.\n",
    "        \"\"\"\n",
    "        for k in scores[\"accuracy@k\"]:\n",
    "            logger.info(\"Accuracy@{}: {:.2f}%\".format(k, scores[\"accuracy@k\"][k] * 100))\n",
    "\n",
    "        for k in scores[\"precision@k\"]:\n",
    "            logger.info(\"Precision@{}: {:.2f}%\".format(k, scores[\"precision@k\"][k] * 100))\n",
    "\n",
    "        for k in scores[\"recall@k\"]:\n",
    "            logger.info(\"Recall@{}: {:.2f}%\".format(k, scores[\"recall@k\"][k] * 100))\n",
    "\n",
    "        for k in scores[\"mrr@k\"]:\n",
    "            logger.info(\"MRR@{}: {:.4f} rank from top\".format(k, scores[\"mrr@k\"][k]))\n",
    "\n",
    "        for k in scores[\"ndcg@k\"]:\n",
    "            logger.info(\"NDCG@{}: {:.4f}%\".format(k, scores[\"ndcg@k\"][k] * 100))\n",
    "\n",
    "        for k in scores[\"map@k\"]:\n",
    "            logger.info(\"MAP@{}: {:.4f} relevant to query\".format(k, scores[\"map@k\"][k] * 100))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = f\"{base_dir}/{subfolder}/embeddings.npy\"\n",
    "query_embeddings_path = f\"{base_dir}/evaluation_dataset_{subfolder}/query_embeddings.npy\"\n",
    "\n",
    "corpus_embeddings = np.load(embeddings_path)\n",
    "query_embeddings = np.load(query_embeddings_path)\n",
    "# convert to tensor\n",
    "corpus_embeddings = torch.from_numpy(corpus_embeddings)\n",
    "query_embeddings = torch.from_numpy(query_embeddings)\n",
    "#normalize for dot product (could not normalize for cosine similarity)\n",
    "corpus_embeddings = F.normalize(corpus_embeddings, p=2, dim=1)\n",
    "query_embeddings = F.normalize(query_embeddings, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------- Similarity function: dot ----------------\n",
      "Accuracy@1: 68.18%\n",
      "Accuracy@3: 82.95%\n",
      "Accuracy@5: 92.05%\n",
      "Accuracy@10: 96.59%\n",
      "Precision@1: 68.18%\n",
      "Precision@3: 30.68%\n",
      "Precision@5: 20.68%\n",
      "Precision@10: 11.25%\n",
      "Recall@1: 56.58%\n",
      "Recall@3: 73.91%\n",
      "Recall@5: 81.25%\n",
      "Recall@10: 85.84%\n",
      "MRR@10: 1.2845 rank from top\n",
      "NDCG@10: 19.7897%\n",
      "MAP@100: 69.4015 relevant to query\n",
      " ->>>>>>>>>>>>>>>>>>> Found 3 queries with no relevant documents in top 10\n",
      "\"Akontacija DDPO junij 2020\"\n",
      "\"Photo ID pickup details for Igor Rendulic\"\n",
      "\"Zoom invitation on April 21, 2021\"\n"
     ]
    }
   ],
   "source": [
    "ir_evaluator = MailioInformationRetrievalEvaluator(ground_truth, corpus_embeddings, query_embeddings, similarity_functions=[SimilarityFunction.DOT_PRODUCT])\n",
    "metrics = ir_evaluator.run()\n",
    "for name, score in metrics.items():\n",
    "    logger.info(f\"----------------- Similarity function: {name} ----------------\")\n",
    "    ir_evaluator.output_scores(score)\n",
    "\n",
    "    logger.info(f\" ->>>>>>>>>>>>>>>>>>> Found {len(ir_evaluator.get_problematic_queries())} queries with no relevant documents in top 10\")\n",
    "    for problem_query_index in ir_evaluator.get_problematic_queries():\n",
    "        logger.info(df_queries.iloc[problem_query_index][\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
