{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| default_exp chunking\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor/workspace/mailio-ai/tools/optimal_embeddings_model/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from langchain_text_splitters import TokenTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from tools.optimal_embeddings_model.data_types.email import Email, MessageType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class CustomEmbeddings:\n",
    "    \"\"\"Embed search docs.\n",
    "\n",
    "    Args:\n",
    "        texts: List of text to embed.\n",
    "\n",
    "    Returns:\n",
    "        List of embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self.model.encode(text).tolist() for text in texts]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# create a chunker object\n",
    "class Chunker:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, chunk_size:int=250, chunk_overlap:int=0):\n",
    "        # self.chunker = SemanticChunker(embeddings=custom_embeddings, breakpoint_threshold_type=threshold_type, breakpoint_threshold_amount=threshold_amount)\n",
    "        # self.chunker = TokenTextSplitter.from_huggingface_tokenizer(tokenizer=tokenizer, chunk_size=chunk_size, chunk_overlap=chunk_overlap, keep_whitespace=True)\n",
    "        self.chunker = SentenceTransformersTokenTextSplitter.from_huggingface_tokenizer(tokenizer=tokenizer, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "    def chunk(self, text:str):\n",
    "        chunks = self.chunker.split_text(text)\n",
    "        return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-multilingual-cased\")\n",
    "max_length = tokenizer.model_max_length\n",
    "chunker = Chunker(tokenizer, chunk_size=max_length - 2, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"No their isnt\", \"Get From Igor Rendulic igorampliogmailcom Sent Friday October 11 2024 113801 AM To Jones Ryan RJONES2amfamcom Subject Re Water leak Hi Ryan Thanks for the info\", \"I wont be filing any claims at this time\", \"One question though Are there any cancellation fees if I switch my insurance\", \"Thank you for your answer in advance\", \"Best Igor On Thu Oct 10 2024 at 3 10 PM Jones Hi Ryan Thanks for the info\", \"I wont be filing any claims at this time\", \"One question though Are there any cancellation fees if I switch my insurance\", \"Thank you for your answer in advance\", \"Best Igor On Thu Oct 10 2024 at 310 PM Jones Ryan wrote At this point its up to you\", \"You have a 1000 deductible\", \"I usually will advise clients to save the home insurance claims for the larger stuff\", \"When you file a claim youll lose the claims free discount\", \"If you have to file another claim usually it will result in an underwriting non renewal\", \"It will then be very difficult to get insurance with another company\", \"Get From Igor Rendulic Sent Thursday October 10 2024 123925 PM To Jones Ryan Subject Re Water leak Hi Ryan Here is the invoice we got for the repairs and the detailed explanation\", \"This doesnt include the fix for the damage on the wall and the floor\", \"Please let me know if any of this is claimable and worth claiming\", \"Thank you Igor On Hi Ryan Here is the invoice we got for the repairs and the detailed explanation\", \"This doesnt include the fix for the damage on the wall and the floor\", \"Please let me know if any of this is claimable and worth claiming\", \"Thank you Igor On Wed Oct 9 2024 at 212 PM Jones Ryan wrote K sounds great\", \"Thank You Your feedback is valuable to us\", \"You may receive a survey and we actively use that feedback to constantly improve our delivery and provide you with the best possible service\", \"Ryan Jones American Family Insurance 8015427041 From Igor Rendulic Sent Wednesday October 9 2024 209 PM To Jones Ryan Subject Re Water leak Ive called the plumbing company\", \"They should get here today sometime\", \"Ill ask them for damage assessment if theyre able to do it\", \"On Wed Oct 9 2024 at 11 56 AM Jones Ryan RJONES2 amfam\", \"com wrote It likely could cover the resulting Ive called the plumbing company\", \"They should get here today sometime\", \"Ill ask them for damage assessment if theyre able to do it\", \"On Wed Oct 9 2024 at 1156 AM Jones Ryan wrote It likely could cover the resulting damage from the break\", \"We usually recommend getting a company over there to assess the damage and see if it would be worth it to file the claim\", \"Would you like a recommendation or would you just like to move forward with the claims process\", \"Thank You Your feedback is valuable to us\", \"You may receive a survey and we actively use that feedback to constantly improve our delivery and provide you with the best possible service\", \"Ryan Jones American Family Insurance 8015427041 From Igor Rendulic Sent Wednesday October 9 2024 827 AM To Jones Ryan Subject Water leak Hi Ryan It appears we have a leak from on the basement water pipes in our home\", \"Does our home insurance cover that\", \"If so how do we go about it\", \"Thank you Igor Rendulic Hi Ryan It appears we have a leak from on the basement water pipes in our home\", \"Does our home insurance cover that\", \"If so how do we go about it\", \"Thank you Igor Rendulic American Family Insurance Company American Family Life Insurance Company American Family Mutual Insurance Company SI\", \"American Standard Insurance Company of Ohio American Standard Insurance Company of Wisconsin Home Office Permanent General Assurance Corporation Permanent General Assurance Corporation of Ohio The General Automobile Insurance Company Inc DBA The General Home Office wholly owned subsidiaries of American Family Mutual Insurance Company SI If you do not want to receive commercial messages from American Family in the future please\", \"If you are not the intended recipient please contact the sender and delete this email any attachments and all copies\"]\n",
    "test_txt =\".\".join(test)\n",
    "chunker.chunk(test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files from data\n",
    "data_dir = '../data'\n",
    "files = [f for f in os.listdir(data_dir) if f.endswith(\".jsonl\") and os.path.isfile(os.path.join(data_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-multilingual-cased\")\n",
    "max_length = tokenizer.model_max_length\n",
    "chunker = Chunker(tokenizer, chunk_size=max_length - 2, chunk_overlap=0)\n",
    "\n",
    "for file in files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        jsonl = f.read()\n",
    "\n",
    "    emails = jsonl.split('\\n')\n",
    "    for i, e in enumerate(emails):\n",
    "        try:\n",
    "            d = json.loads(e)\n",
    "            email = Email.from_dict(d)\n",
    "            if len(email.sentences) > 0:\n",
    "                text = \".\".join(email.sentences)\n",
    "                chunks = chunker.chunk(text)\n",
    "                if len(chunks) > 2:\n",
    "                    print(f\"Email {i} has {len(chunks)} chunks\")\n",
    "                    for ch in chunks:\n",
    "                        print(ch)\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Error in email {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
