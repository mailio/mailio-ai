# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_create_embeddings.ipynb.

# %% auto 0
__all__ = ['Embedder']

# %% ../nbs/02_create_embeddings.ipynb 1
from typing import List, Optional
import torch
from transformers import PreTrainedModel, PreTrainedTokenizer, BatchEncoding
from transformers import AutoTokenizer, DistilBertModel
import sys
import torch.nn.functional as F

sys.path.append("..")  # Adds the parent directory to sys path

from .chunking import Chunker

# %% ../nbs/02_create_embeddings.ipynb 2
class Embedder:

    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):
        self.model = model
        self.model.eval()
        self.tokenizer = tokenizer
        self.max_length = self.model.config.max_position_embeddings  # Model-specific max lengt
        self.chunker = Chunker(tokenizer, chunk_size=self.max_length-2, chunk_overlap=0)
    
    def embed(self, text:List[str]) -> torch.Tensor:
        """
        Generate embeddings for a list of texts.
        
        Args:
            text (List[str]): List of input texts.
            chunk_size (Optional[int]): If text is too long, split it into chunks of this size. If None, no chunking is done.
        
        Returns:
            numpy.ndarray: Embeddings for the input texts.
        """
        
        # Tokenize the input text
        chunks = self.chunker.chunk(".".join(text))
        embeddings = []

        input_texts = []
        for ch in chunks:
            input_texts.append(ch.page_content)
        
        inputs = self.tokenizer(input_texts, padding=True, truncation=True, return_tensors="pt")
        inputs = inputs.to(self.model.device)

        with torch.no_grad():
            # Forward pass
            outputs = self.model(**inputs)
        
        #TODO! remove the mean pooling. Might not be harmful for quality of results
        # Perform pooling. This will convert the output to a tensor of shape (batch_size, hidden_size)
        pooled_output = self.mean_pooling(outputs, inputs['attention_mask'])

        norm = F.normalize(pooled_output, p=2, dim=1)
            
        # Convert to numpy array and return
        # Move tensors in the list to CPU and convert them to numpy arrays
        return [norm.cpu().numpy()]

    def mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0] #First element of model_output contains all token embeddings
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

