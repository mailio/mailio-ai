{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp create_embeddings\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor/workspace/mailio-ai/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, BatchEncoding\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(\"..\")  # Adds the parent directory to sys path\n",
    "\n",
    "from mailio_ai_libs.chunking import Chunker\n",
    "from data_types.email import Email, MessageType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Embedder:\n",
    "\n",
    "    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = self.model.config.max_position_embeddings  # Model-specific max lengt\n",
    "        self.chunker = Chunker(tokenizer, chunk_size=self.max_length-2, chunk_overlap=0)\n",
    "    \n",
    "    def embed(self, text:List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts.\n",
    "        \n",
    "        Args:\n",
    "            text (List[str]): List of input texts.\n",
    "            chunk_size (Optional[int]): If text is too long, split it into chunks of this size. If None, no chunking is done.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Embeddings for the input texts.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        chunks = self.chunker.chunk(\".\".join(text))\n",
    "        embeddings = []\n",
    "\n",
    "        input_texts = []\n",
    "        for ch in chunks:\n",
    "            input_texts.append(ch.page_content)\n",
    "        \n",
    "        inputs = self.tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        #TODO! remove the mean pooling. Might not be harmful for quality of results\n",
    "        # Perform pooling. This will convert the output to a tensor of shape (batch_size, hidden_size)\n",
    "        pooled_output = self.mean_pooling(outputs, inputs['attention_mask'])\n",
    "\n",
    "        norm = F.normalize(pooled_output, p=2, dim=1)\n",
    "            \n",
    "        # Convert to numpy array and return\n",
    "        # Move tensors in the list to CPU and convert them to numpy arrays\n",
    "        return [norm.cpu().numpy()]\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "embedder = Embedder(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedder.embed([\"Hello, world!\", \"this is it\"])\n",
    "print(embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing emails for emails_inbox.jsonl:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2174/2694 [01:59<00:28, 18.19it/s]\n"
     ]
    },
    {
     "ename": "WrongTypeError",
     "evalue": "wrong value type for field \"subject\" - should be \"typing.Optional[str]\" instead of value \"[\"16x20 Canvas only $14.99 ðŸ“£ Don't Miss This!\", '']\" of type \"list\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWrongTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m data \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;28;01mif\u001b[39;00m d]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm(data, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing emails for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(file)):\n\u001b[0;32m---> 19\u001b[0m     email \u001b[38;5;241m=\u001b[39m \u001b[43mEmail\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     text \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m email\u001b[38;5;241m.\u001b[39msender_name:\n",
      "File \u001b[0;32m~/workspace/mailio-ai/nbs/../data_types/email.py:29\u001b[0m, in \u001b[0;36mEmail.from_dict\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_dict\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mMessageType\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/mailio-ai/.venv/lib/python3.10/site-packages/dacite/core.py:69\u001b[0m, in \u001b[0;36mfrom_dict\u001b[0;34m(data_class, data, config)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mcheck_types \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_instance(value, field_type):\n\u001b[0;32m---> 69\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m WrongTypeError(field_path\u001b[38;5;241m=\u001b[39mfield\u001b[38;5;241m.\u001b[39mname, field_type\u001b[38;5;241m=\u001b[39mfield_type, value\u001b[38;5;241m=\u001b[39mvalue)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mWrongTypeError\u001b[0m: wrong value type for field \"subject\" - should be \"typing.Optional[str]\" instead of value \"[\"16x20 Canvas only $14.99 ðŸ“£ Don't Miss This!\", '']\" of type \"list\""
     ]
    }
   ],
   "source": [
    "# list all files from data\n",
    "data_dir = '../data'\n",
    "files = os.listdir(data_dir)\n",
    "\n",
    "output_folder = \"../data/embeddings_distilbert_base_uncased_mean_pooling\"\n",
    "os.makedirs(output_folder, exist_ok=True)   \n",
    "\n",
    "# create embeddings\n",
    "for file in files:\n",
    "\n",
    "    all_embeddings_index = []\n",
    "    all_embeddings = []\n",
    "\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        jsonl = f.read()\n",
    "    data = jsonl.split('\\n')\n",
    "    data = [json.loads(d) for d in data if d]\n",
    "    for item in tqdm(data, desc=\"Processing emails for {}\".format(file)):\n",
    "        email = Email.from_dict(item)\n",
    "        text = []\n",
    "        if email.sender_name:\n",
    "            text.append(f\"sent from {email.sender_name}\")\n",
    "        if email.subject:\n",
    "            text = [email.subject]\n",
    "        if len(email.sentences) > 0:\n",
    "            text.extend(email.sentences)\n",
    "        \n",
    "        if len(text) == 0: # nothing to embed\n",
    "            continue\n",
    "        \n",
    "        embeddings = embedder.embed(text)\n",
    "\n",
    "        all_embeddings.append(embeddings)\n",
    "        # add to index number of == len(embeddings) \n",
    "        all_embeddings_index.extend([email.message_id] * len(embeddings))\n",
    "\n",
    "    # Save the embeddings to a file\n",
    "    np_emebddings = np.concatenate(all_embeddings, axis=1)\n",
    "    embeddings_index = np.array(all_embeddings_index)\n",
    "\n",
    "    embeddings_filename = file.split('.')[0]\n",
    "    with open(os.path.join(output_folder, f\"{emebddings_filename}.npy\"), 'wb') as f:\n",
    "        np.save(f, np_emebddings)\n",
    "\n",
    "    with open(os.path.join(output_folder, f\"{emebddings_filename}_index.npy\"), 'wb') as f:\n",
    "        np.save(f, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
